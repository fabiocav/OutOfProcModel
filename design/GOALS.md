# Worker Model Refactor - Goals and Motivation

## Executive Summary

The current Azure Functions architecture tightly couples the **Runtime (Script Host)** and **Worker Process** into a single container that scales as one unit. This design creates significant inefficiencies in resource utilization, limits multi-tenancy capabilities, and increases cold start times.

The **New Worker Model** decouples these components, allowing multiple lightweight worker containers to connect to a single Runtime instance. This architectural shift enables:

- **3-5x improvement in container density** (multiple customers per Runtime)
- **40-60% reduction in memory footprint** per customer application
- **Improved cold start times** through placeholder worker pooling
- **Better resource isolation** between customer applications
- **More efficient scaling** by independently scaling workers vs. Runtime

---

## Current Architecture: Problems and Inefficiencies

### Current Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Container Instance              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚      Runtime (Script Host)        â”‚  â”‚
â”‚  â”‚  - gRPC Server                    â”‚  â”‚
â”‚  â”‚  - WebJobs SDK                    â”‚  â”‚
â”‚  â”‚  - Trigger Listeners              â”‚  â”‚
â”‚  â”‚  - ScaleController Client         â”‚  â”‚
â”‚  â”‚  - Configuration                  â”‚  â”‚
â”‚  â”‚                                   â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚   Worker Process (Child)    â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  - Language Runtime         â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  - Customer Code            â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  - gRPC Client              â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                         â”‚
â”‚  Both components scale together         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Characteristics:**
- **Single-tenant**: One container per customer application
- **Process hierarchy**: Runtime starts and monitors worker as child process
- **Tightly coupled**: Both components must scale together
- **In-process communication**: gRPC over localhost

---

## Problem 1: Inefficient Resource Utilization

### Memory Overhead

Each container instance requires:

| Component | Memory Usage | Purpose |
|-----------|--------------|---------|
| **Runtime Base** | ~150-200 MB | .NET Runtime, Script Host, WebJobs SDK |
| **Trigger Extensions** | ~50-100 MB | Storage, Event Hub, Service Bus SDKs |
| **Configuration** | ~20-30 MB | Connection strings, app settings |
| **Monitoring/Logging** | ~30-50 MB | Application Insights, structured logging |
| **Worker Runtime** | ~100-300 MB | Python, Node.js, .NET, Java runtime |
| **Customer Code** | ~50-200 MB | Dependencies, libraries, app code |
| **Total per App** | **~400-880 MB** | Memory per customer application |

**Problem**: Most of this overhead is **duplicated across every customer**, even though the Runtime components are largely identical.

### Example: 10 Customer Applications

**Current Model:**
```
10 Containers Ã— 500 MB average = 5,000 MB (5 GB)
```

**Potential with Shared Runtime:**
```
1 Runtime (200 MB) + 10 Workers (150 MB each) = 1,700 MB (1.7 GB)
Savings: 66% reduction in memory usage
```

---

## Problem 2: Scaling Inefficiencies

### Coupled Scaling

When a customer application needs to scale out:

```
Customer App needs 3 instances
â†“
Must deploy 3 complete containers
â†“
3 Ã— Runtime + 3 Ã— Worker = 3x full overhead
```

**Issues:**
1. **Over-provisioning**: Runtime components scaled unnecessarily
2. **Slow scaling**: Large container images take longer to deploy
3. **Wasted resources**: Most Runtime components sit idle
4. **Cost**: Higher compute costs due to redundant components

### Real-World Example: Timer Trigger Application

```
Application: Runs once per hour
Problem: Runtime + Worker running 24/7
Utilization: ~0.02% (1 min active / 60 min idle)
Waste: 99.98% of resources sit idle
```

With new model:
- Runtime can serve multiple applications (better amortization)
- Worker can be stopped when not needed (scale to zero scenarios)

---

## Problem 3: Cold Start Performance

### Current Cold Start Sequence

```
1. Container provisioning          ~5-10s
2. Runtime initialization          ~3-5s
   - Load .NET assemblies
   - Initialize WebJobs SDK
   - Start trigger listeners
3. Worker process start            ~2-5s
   - Start language runtime
   - Load dependencies
4. Function loading                ~1-3s
   - Load customer code
5. First invocation                ~0.1s

Total: ~11-23s per cold start
```

**Problems:**
- Steps 1-2 are **identical across all customers** but repeated every time
- Cannot pre-warm or pool Runtime instances effectively
- Large container images slow down provisioning

---

## Problem 4: Limited Multi-Tenancy

### Current Isolation Model

Each customer requires their own container, preventing:

- **Resource sharing**: Cannot amortize Runtime costs
- **Density improvements**: Limited by container overhead
- **Efficient placeholder mode**: Must maintain full containers as placeholders

### Example: Placeholder Mode Today

To support fast specialization for 5 languages:

```
5 Placeholder Containers Ã— 400 MB = 2,000 MB (2 GB)
```

Only to wait idle until a customer is assigned!

---

## Problem 5: Operational Complexity

### Process Monitoring

The Runtime must:
- Start worker as child process
- Monitor worker health
- Restart worker on failures
- Manage worker lifecycle
- Handle process crashes

**Code Example (Current):**

```csharp
// Runtime must manage worker process lifecycle
public class GrpcWorkerChannel
{
    private Process _workerProcess;
    
    public async Task StartWorkerAsync()
    {
        _workerProcess = new Process
        {
            StartInfo = new ProcessStartInfo
            {
                FileName = "python",
                Arguments = "worker.py",
                // ... configuration
            }
        };
        
        _workerProcess.Start();
        
        // Monitor for crashes
        _workerProcess.Exited += OnWorkerExited;
        
        // Monitor for hangs
        _processMonitor.RegisterChildProcess(_workerProcess);
    }
    
    private void OnWorkerExited(object sender, EventArgs e)
    {
        // Handle worker crash - restart or fail
        if (_workerProcess.ExitCode != 0)
        {
            // Restart logic...
        }
    }
}
```

**Problems:**
- Runtime complexity increases
- Process boundary issues (signals, environment variables)
- Platform-specific behavior (Windows vs Linux)
- Difficult to debug cross-process issues

---

## New Architecture: Independent Scaling

### New Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Runtime Container                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           Runtime (Script Host)                  â”‚  â”‚
â”‚  â”‚  - gRPC Server (listening)                       â”‚  â”‚
â”‚  â”‚  - JobHost Manager                               â”‚  â”‚
â”‚  â”‚  - Worker Registry                               â”‚  â”‚
â”‚  â”‚  - Message Routing                               â”‚  â”‚
â”‚  â”‚                                                  â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚  â”‚
â”‚  â”‚  â”‚ JobHost 1    â”‚  â”‚ JobHost 2    â”‚            â”‚  â”‚
â”‚  â”‚  â”‚ (App A)      â”‚  â”‚ (App B)      â”‚            â”‚  â”‚
â”‚  â”‚  â”‚ WebJobs SDK  â”‚  â”‚ WebJobs SDK  â”‚            â”‚  â”‚
â”‚  â”‚  â”‚ Triggers     â”‚  â”‚ Triggers     â”‚            â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–²
                          â”‚ gRPC over network
                          â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                â”‚                â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚ Worker 1 â”‚    â”‚ Worker 2 â”‚    â”‚ Worker 3 â”‚
    â”‚ (App A)  â”‚    â”‚ (App B)  â”‚    â”‚ (App A)  â”‚
    â”‚ Python   â”‚    â”‚ Python   â”‚    â”‚ Python   â”‚
    â”‚ 150 MB   â”‚    â”‚ 150 MB   â”‚    â”‚ 150 MB   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Characteristics:**
- **Multi-tenant Runtime**: One Runtime serves multiple applications
- **Independent workers**: Workers are separate containers
- **Network communication**: gRPC over network (not localhost)
- **Independent scaling**: Scale workers without scaling Runtime

---

## Benefits: Capacity Improvements

### Benefit 1: Memory Density

**Before:**
```
Node on Azure: 7 GB available
Current model: 500 MB per app
Capacity: ~14 apps per node
```

**After:**
```
Node on Azure: 7 GB available
1 Runtime: 200 MB
Workers: 150 MB each
Capacity: ~45 apps per node

Improvement: 3.2x density increase
```

### Benefit 2: Cost Reduction

**Scenario**: 100 customer applications, each with 2 instances

**Current Model:**
```
200 containers Ã— 500 MB = 100 GB memory
Azure cost: ~$500/month (example pricing)
```

**New Model:**
```
10 Runtimes Ã— 200 MB = 2 GB
200 Workers Ã— 150 MB = 30 GB
Total: 32 GB memory

Azure cost: ~$160/month
Savings: 68% reduction
```

### Benefit 3: Improved Cold Start

**With Placeholder Workers:**

```
1. Runtime already running          0s (already up)
2. Worker already connected         0s (placeholder pool)
3. Specialization                   ~2-3s
   - Stop placeholder JobHost
   - Create customer JobHost
   - Load customer code
4. First invocation                 ~0.1s

Total: ~2-3s cold start

Improvement: 70-85% faster than current
```

### Benefit 4: Better Scale-Out

**Scenario**: Application needs to scale from 1 to 10 instances

**Current Model:**
```
Deploy 9 new containers
- 9 Ã— Runtime initialization: ~27-45s
- 9 Ã— Worker initialization: ~18-45s
- Container image pull: ~5-10s each

Total scale-out time: ~50-100s
```

**New Model:**
```
Connect 9 new workers to existing Runtime
- Worker container pull: ~2-5s (smaller image)
- Worker initialization: ~2-5s
- No Runtime initialization needed

Total scale-out time: ~4-10s

Improvement: 90% faster scale-out
```

---

## Benefits: Multi-Tenancy Support

### Efficient Placeholder Pools

**Current Model (5 languages):**
```
5 Placeholder Containers
Each: Runtime (200 MB) + Worker (150 MB) = 350 MB
Total: 1,750 MB

Waste: Entire Runtime sitting idle in each container
```

**New Model (5 languages):**
```
1 Runtime: 200 MB (shared)
5 Placeholder Workers: 5 Ã— 150 MB = 750 MB
Total: 950 MB

Savings: 46% memory reduction
Benefit: Runtime is shared, always doing work
```

### Higher Density Per Node

**Example: 4-core Azure node with 14 GB RAM**

**Current Model:**
```
Max apps: ~28 (500 MB each)
Constraint: Memory limited
```

**New Model:**
```
Runtimes: 4 (one per core) = 800 MB
Workers: 88 (150 MB each) = 13.2 GB
Max apps: ~88

Improvement: 3.1x increase in apps per node
```

---

## Changes Required in New Design

### Change 1: Worker Process Management

**Current Behavior:**
```csharp
// Runtime starts and monitors worker
public class GrpcWorkerChannel
{
    private Process _workerProcess;
    
    public async Task StartWorkerAsync()
    {
        // Runtime STARTS the worker process
        _workerProcess = Process.Start("python", "worker.py");
        
        // Runtime MONITORS worker health
        _workerProcess.Exited += OnWorkerExited;
        _processMonitor.RegisterChildProcess(_workerProcess);
    }
}
```

**New Behavior:**
```csharp
// Worker connects to Runtime (Runtime doesn't start it)
public class FunctionRpcService
{
    public override async Task EventStream(...)
    {
        // Worker initiates connection
        // Runtime just accepts and registers it
        var startStream = await requestStream.ReadNext();
        await _workerRegistry.RegisterWorkerAsync(startStream);
        
        // No process monitoring - worker is independent
        // Infrastructure handles worker lifecycle
    }
}
```

**Impact:**
- âœ… **Simpler Runtime code**: No process management logic
- âœ… **Better separation**: Infrastructure manages containers
- âš ï¸ **Different health model**: Must rely on gRPC connection health
- âš ï¸ **Worker responsibility**: Workers must self-monitor and reconnect

---

### Change 2: Health Monitoring

**Current Approach:**
```csharp
public class DefaultHostProcessMonitor
{
    private List<ProcessMonitor> _childProcessMonitors;
    
    public void RegisterChildProcess(Process process)
    {
        // Monitor CPU, memory of worker process
        var monitor = new ProcessMonitor(process);
        _childProcessMonitors.Add(monitor);
        
        // Track if worker is hung, high CPU, high memory
    }
}
```

**New Approach:**
```csharp
public class WorkerHealthMonitor
{
    public async Task MonitorWorkerHealthAsync(string workerId)
    {
        // Health based on gRPC connection and heartbeats
        var worker = await _workerRegistry.GetWorkerAsync(workerId);
        
        // Check last heartbeat
        if (DateTime.UtcNow - worker.LastHeartbeat > TimeSpan.FromSeconds(30))
        {
            // Worker is unhealthy - mark as disconnected
            await _workerRegistry.UpdateWorkerStateAsync(
                workerId, 
                WorkerStatus.Disconnected);
        }
        
        // Cannot monitor CPU/memory directly
        // Infrastructure must handle that
    }
}
```

**Impact:**
- âš ï¸ **Reduced visibility**: Runtime can't directly monitor worker resources
- âœ… **Infrastructure responsibility**: Kubernetes/ACI handles resource limits
- âš ï¸ **Heartbeat required**: Need periodic health checks via gRPC
- âœ… **Better isolation**: Worker resource issues don't affect Runtime

---

### Change 3: Environment Variables and Configuration

**Current Approach:**
```csharp
// Runtime controls worker environment completely
var workerProcess = new Process
{
    StartInfo = new ProcessStartInfo
    {
        EnvironmentVariables =
        {
            ["AzureWebJobsStorage"] = connectionString,
            ["FUNCTIONS_WORKER_RUNTIME"] = "python",
            ["ApplicationInsightsAgent_EXTENSION_VERSION"] = "~2",
            // ... 50+ environment variables
        }
    }
};
```

**New Approach:**
```csharp
// Worker receives configuration via gRPC messages
var reloadRequest = new FunctionEnvironmentReloadRequest
{
    EnvironmentVariables = 
    {
        { "AzureWebJobsStorage", connectionString },
        { "FUNCTIONS_WORKER_RUNTIME", "python" },
        // ... configuration
    },
    FunctionAppDirectory = "/home/site/wwwroot"
};

await SendToWorkerAsync(reloadRequest);
```

**Impact:**
- âš ï¸ **Different mechanism**: Configuration via gRPC instead of env vars
- âœ… **More flexible**: Can update configuration without restarting worker
- âš ï¸ **Compatibility**: Need to ensure worker handles dynamic configuration
- âœ… **Security**: Sensitive config not visible in process environment

---

### Change 4: Logging and Diagnostics

**Current Approach:**
```csharp
// Runtime captures worker stdout/stderr directly
_workerProcess.OutputDataReceived += (sender, args) =>
{
    _logger.LogInformation("[Worker] {Output}", args.Data);
};

_workerProcess.ErrorDataReceived += (sender, args) =>
{
    _logger.LogError("[Worker] {Error}", args.Data);
};
```

**New Approach:**
```csharp
// Worker sends logs via RpcLog messages
private async Task HandleRpcLogAsync(RpcLog log)
{
    var logLevel = ConvertLogLevel(log.Level);
    
    _logger.Log(
        logLevel,
        "[Worker {WorkerId}] [{Category}] {Message}",
        log.WorkerId,
        log.Category,
        log.Message);
}
```

**Impact:**
- âš ï¸ **Structured only**: No raw stdout/stderr capture
- âœ… **Better structure**: Logs include category, level, metadata
- âš ï¸ **Worker responsibility**: Worker must send all logs via RPC
- âœ… **Consistent format**: All logs flow through same channel

---

### Change 5: Crash Recovery

**Current Approach:**
```csharp
private void OnWorkerExited(object sender, EventArgs e)
{
    if (_workerProcess.ExitCode != 0)
    {
        _logger.LogError("Worker crashed with exit code {ExitCode}", 
            _workerProcess.ExitCode);
        
        // Restart worker automatically
        _ = Task.Run(async () => 
        {
            await Task.Delay(1000);
            await StartWorkerAsync();
        });
    }
}
```

**New Approach:**
```csharp
// Infrastructure handles worker restart
// Runtime just detects disconnection
public async Task OnWorkerDisconnectedAsync(string workerId)
{
    _logger.LogWarning("Worker {WorkerId} disconnected", workerId);
    
    // Mark worker as disconnected
    await _workerRegistry.UpdateWorkerStateAsync(
        workerId,
        WorkerStatus.Disconnected);
    
    // Remove from active pool
    await _workerRegistry.RemoveWorkerAsync(workerId);
    
    // Infrastructure (K8s, ACI) will restart worker container
    // Worker will reconnect when ready
    // No explicit restart logic needed
}
```

**Impact:**
- âœ… **Simpler Runtime**: No restart logic needed
- âš ï¸ **Depends on infrastructure**: Must configure restart policies
- âš ï¸ **Reconnection handling**: Worker must re-register on restart
- âœ… **Better separation**: Runtime doesn't need to manage worker lifecycle

---

### Change 6: Worker Assignment

**Current Approach:**
```csharp
// One worker per function app, always
// Worker is started by Runtime for specific app
public async Task StartWorkerForAppAsync(ApplicationDefinition appDef)
{
    var workerProcess = await StartWorkerProcessAsync(appDef);
    // Worker is dedicated to this app
}
```

**New Approach:**
```csharp
// Dynamic worker assignment from pool
public async Task AssignWorkerToAppAsync(ApplicationDefinition appDef)
{
    // Find available worker (placeholder or new)
    var worker = await _workerRegistry.GetAvailableWorkerAsync(
        requiredLanguage: appDef.Language,
        requiredVersion: appDef.LanguageVersion);
    
    if (worker == null)
    {
        // Request new worker from infrastructure
        await _workerProvisioningService.RequestWorkerAsync(
            appDef.Language,
            appDef.LanguageVersion);
        
        // Wait for worker to connect
        worker = await WaitForWorkerConnectionAsync();
    }
    
    // Specialize worker for this app
    await SpecializeWorkerAsync(worker, appDef);
}
```

**Impact:**
- âœ… **Worker pooling**: Can reuse workers across apps
- âœ… **Better resource usage**: Workers can be reassigned
- âš ï¸ **Complex coordination**: Need worker provisioning service
- âš ï¸ **State management**: Must track worker assignments

---

## Migration Strategy

### Phase 1: Proof of Concept (Current)

- âœ… Implement channel-based communication
- âœ… Separate JobHost per application
- âœ… Worker connects to Runtime (not started by it)
- ğŸ”„ Test multi-tenant scenarios

### Phase 2: Production Readiness

- âš ï¸ Remove process monitoring code from Runtime
- âš ï¸ Implement heartbeat/health check protocol
- âš ï¸ Add worker reconnection logic
- âš ï¸ Update configuration delivery mechanism
- âš ï¸ Implement placeholder worker pool management

### Phase 3: Infrastructure Integration

- âš ï¸ Update deployment models (separate Runtime/Worker containers)
- âš ï¸ Configure container orchestration (K8s, ACI)
- âš ï¸ Implement worker provisioning service
- âš ï¸ Update autoscale rules (separate for Runtime vs Workers)

### Phase 4: Optimization

- âš ï¸ Fine-tune Runtime capacity (how many JobHosts per Runtime)
- âš ï¸ Optimize worker image size
- âš ï¸ Implement intelligent worker assignment
- âš ï¸ Add multi-region support

---

## Success Metrics

### Resource Efficiency

| Metric | Current | Target | Improvement |
|--------|---------|--------|-------------|
| Memory per app | 400-880 MB | 150-200 MB | 60-75% reduction |
| Apps per node | 14-28 | 45-90 | 3-4x increase |
| Cold start time | 11-23s | 2-5s | 70-85% faster |
| Scale-out time | 50-100s | 4-10s | 90% faster |

### Cost Efficiency

| Scenario | Current Cost | New Cost | Savings |
|----------|--------------|----------|---------|
| 100 apps, 2 instances | $500/mo | $160/mo | 68% |
| 1000 apps | $5000/mo | $1600/mo | 68% |

### Operational

| Metric | Target |
|--------|--------|
| Worker reconnection time | <1s |
| Health check interval | 15-30s |
| JobHost creation time | <500ms |
| Message routing latency | <5ms |

---

## Risks and Mitigations

### Risk 1: Network Latency

**Concern**: gRPC over network slower than localhost

**Mitigation**:
- Workers and Runtime in same Azure region (< 1ms RTT)
- Use HTTP/2 connection pooling
- Benchmark shows <5ms overhead acceptable

### Risk 2: Worker Orphaning

**Concern**: Worker disconnects but infrastructure doesn't restart

**Mitigation**:
- Implement heartbeat protocol (worker sends, Runtime expects)
- Infrastructure monitors worker health independently
- Timeout and cleanup orphaned workers

### Risk 3: Backward Compatibility

**Concern**: Existing workers expect to be started by Runtime

**Mitigation**:
- Phase migration: support both models temporarily
- Update worker SDKs to support connection mode
- Document migration path for custom workers

### Risk 4: Debugging Complexity

**Concern**: Harder to debug across network boundary

**Mitigation**:
- Enhanced telemetry and correlation IDs
- gRPC message tracing/logging
- Local development support (both models)

---

## Conclusion

The new Worker Model represents a fundamental architectural improvement that:

âœ… **Reduces memory usage by 60-75%** through shared Runtime infrastructure

âœ… **Increases node density by 3-4x** enabling better resource utilization

âœ… **Improves cold start by 70-85%** through placeholder worker pooling

âœ… **Enables true multi-tenancy** with isolated JobHosts per customer

âœ… **Simplifies Runtime** by removing process management complexity

The changes required are manageable and primarily involve:
- Removing worker process monitoring from Runtime
- Implementing gRPC-based health checks
- Updating configuration delivery
- Relying on infrastructure for worker lifecycle

The benefits far outweigh the migration effort, and this architecture positions Azure Functions for better scalability, efficiency, and cost-effectiveness.
